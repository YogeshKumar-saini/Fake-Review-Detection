# Fake Review Detection

*Uncovering authenticity, one review at a time.*

## ğŸ“– Overview

Fake reviews harm online trust and decision-making. This project uses NLP feature engineering and machine learning models to systematically detect computer-generated (fake) reviews from human-written ones.

We leverage Scikit-Learn models, NLTK/Spacy preprocessing, and track experiments using MLflow, deploying our final model via a Dockerized Streamlit app for live testing.

---

## ğŸ“‚ Project Organization

```
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile           <- Commands like `make data`, `make train`
â”œâ”€â”€ README.md          <- This file
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ external       <- Third-party raw data
â”‚Â Â  â”œâ”€â”€ interim        <- Transformed intermediate data
â”‚Â Â  â”œâ”€â”€ processed      <- Final dataset for modeling
â”‚Â Â  â””â”€â”€ raw            <- Original immutable data dump
â”œâ”€â”€ docs               <- Sphinx documentation
â”œâ”€â”€ models             <- Trained models and predictions
â”œâ”€â”€ notebooks          <- Jupyter notebooks for EDA and testing
â”œâ”€â”€ references         <- Data dictionaries and manuals
â”œâ”€â”€ reports            <- Generated analysis and figures
â”œâ”€â”€ requirements.txt   <- Dependency list
â”œâ”€â”€ setup.py           <- Pip installable project structure
â”œâ”€â”€ src                <- Source code
â”‚Â Â  â”œâ”€â”€ data           <- Data collection and generation
â”‚Â Â  â”œâ”€â”€ features       <- Feature engineering scripts
â”‚Â Â  â”œâ”€â”€ models         <- Training and prediction scripts
â”‚Â Â  â””â”€â”€ visualization  <- EDA and result visualizations
â””â”€â”€ tox.ini            <- Testing configurations
```

---

## ğŸ› ï¸ Installation

### Prerequisites

* Python 3.8+
* Docker (for deployment)
* Git

### Steps

```bash
git clone https://github.com/YogeshKumar-saini/Fake-Review-Detection.git
cd fake-review-detection

python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

pip install -r requirements.txt
```

---

## ğŸš€ Usage

### Preprocess Data

```bash
make data
```

### Train the Model

```bash
make train
```

### Explore Notebooks

```bash
jupyter notebook notebooks/
```

### Launch Streamlit App Locally

```bash
docker build -t fake-review-app .
docker run -p 8501:8501 fake-review-app
```

Access the app at [http://localhost:8501](http://localhost:8501).

---

## â˜ï¸ Deployment on AWS

Deploy the Streamlit app on an AWS EC2 instance:

```bash
docker run -d -p 80:8501 yogesh1090/fake-review-app:latest
```


Access via: `http://13.53.99.130:8501/`

Ensure your EC2 security group allows inbound traffic on port 80.

---

## ğŸ“Š Experiment Tracking with MLflow

Track experiments and performance:

```bash
mlflow ui
```

Access the MLflow dashboard at [http://localhost:5000](http://localhost:5000).

---

## ğŸ§© Dataset

* Binary classification dataset of reviews labeled as `fake` or `real`.
* Source: Public datasets like Yelp, Amazon, or custom scraping.
* Includes review text, optional user metadata, and labels.

---

## ğŸ› ï¸ Feature Engineering

* **Text preprocessing:** tokenization, stop-word removal, lemmatization.
* **Feature extraction:** TF-IDF vectors, sentiment scores, review length, readability scores, POS tagging.
* Potential advanced embeddings using BERT for future enhancement.

---

## ğŸ¤– Model Details

* Models tested: Logistic Regression, Random Forest, SVM.
* Evaluation metrics: Accuracy, F1-score, ROC-AUC.
* Best model persisted via Pickle and tracked on MLflow.

---

## ğŸ“ˆ Results

* Achieved \~90% accuracy on validation data.
* ROC-AUC consistently above 0.92.
* Full experiment logs available on MLflow.

---

## ğŸ–¥ï¸ Streamlit App Features

* Input a review to check if it is `fake` or `real` with confidence probability.
* Display dataset statistics and charts.
* Option to view EDA and model performance graphs.

---

## ğŸ› ï¸ Technologies Used

* Python 3.8+
* Scikit-Learn
* NLTK / Spacy
* MLflow
* Streamlit
* Docker
* Pandas / NumPy

---

## ğŸ“š Resources

* [MLflow Documentation](https://mlflow.org/)
* [Streamlit Documentation](https://docs.streamlit.io/)
* [Scikit-Learn Documentation](https://scikit-learn.org/stable/)
* [Cookiecutter Data Science](https://drivendata.github.io/cookiecutter-data-science/)
* [NLTK Book](https://www.nltk.org/book/)

---

## ğŸ¤ Contributing

1. Fork the repository.
2. Create a feature branch (`git checkout -b feature/your-feature`).
3. Commit changes (`git commit -m 'Add your feature'`).
4. Push to your branch (`git push origin feature/your-feature`).
5. Open a pull request.

For major changes, please open an issue to discuss your proposal first.

---

## ğŸ“œ License

This project is licensed under the MIT License.

---

## ğŸ“¬ Contact

Maintained by Yogesh Saini.

For inquiries, please open an issue or email at [yksaini1090@gmail.com](mailto:yksaini1090@gmail.com).

---


> **â€œUncovering authenticity, one review at a time.â€**
