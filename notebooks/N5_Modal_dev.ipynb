{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install -q dagshub mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "import dagshub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"YogeshKumar-saini/Fake-Review-Detection\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"YogeshKumar-saini/Fake-Review-Detection\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository YogeshKumar-saini/Fake-Review-Detection initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository YogeshKumar-saini/Fake-Review-Detection initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dagshub.init(repo_owner='YogeshKumar-saini', repo_name='Fake-Review-Detection', mlflow=True)\n",
    "\n",
    "# Set tracking URI\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow\")\n",
    "\n",
    "\n",
    "client = MlflowClient()\n",
    "default_experiment = client.get_experiment_by_name(\"Fake Review Detection\")\n",
    "\n",
    "runs = client.search_runs(experiment_ids=[default_experiment.experiment_id])\n",
    "\n",
    "for run in runs:\n",
    "    try:\n",
    "        client.delete_run(run.info.run_id)\n",
    "        print(f\"Deleted run {run.info.run_id} from experiment {default_experiment.name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not delete run {run.info.run_id}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for '/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_lemmatization_features.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "\u001b[31m2025/03/19 22:38:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run LogisticRegression_on_preprocessed_lemmatization_features.csv at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0/runs/72a9398d65bd4f8a998f7cc76e3b259a\n",
      "üß™ View experiment at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for '/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_lemmatization_features.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "\u001b[31m2025/03/19 22:40:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run RandomForest_on_preprocessed_lemmatization_features.csv at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0/runs/d23b623cc1514a5cb41f4ceeb1f02564\n",
      "üß™ View experiment at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for '/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_lemmatization_features.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "\u001b[31m2025/03/19 22:47:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run SVC_on_preprocessed_lemmatization_features.csv at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0/runs/9419da789ef64251bf024f77cd199b84\n",
      "üß™ View experiment at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for '/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_lemmatization_features.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n",
      "\u001b[31m2025/03/19 22:50:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run XGBoost_on_preprocessed_lemmatization_features.csv at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0/runs/b2d916b50b5b4ed8a0d730a9ac5cbea7\n",
      "üß™ View experiment at: https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: Failed to determine whether UCVolumeDatasetSource can resolve source information for '/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_lemmatization_features.csv'. Exception: \n",
      "  return _dataset_source_registry.resolve(\n",
      "/home/yogesh/python_lab/python_env/lib/python3.10/site-packages/mlflow/data/dataset_source_registry.py:149: UserWarning: The specified dataset source can be interpreted in multiple ways: LocalArtifactDatasetSource, LocalArtifactDatasetSource. MLflow will assume that this is a LocalArtifactDatasetSource source.\n",
      "  return _dataset_source_registry.resolve(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.keras\n",
    "import mlflow.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/YogeshKumar-saini/Fake-Review-Detection.mlflow\")\n",
    "mlflow.set_experiment(\"Fake Review Detection\")\n",
    "\n",
    "\n",
    "\n",
    "feature_files = [\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_lemmatization_features.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_no_stopwords_features.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_no_stopwords_no_lemmatization_features.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_stemming_features.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/Feature-engineered/preprocessed_stemming_no_stopwords_features.csv\"\n",
    "]\n",
    "\n",
    "embedding_files = [\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_lemmatization_bert.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_lemmatization_glove.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_lemmatization_tfidf.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_no_stopwords_bert.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_no_stopwords_glove.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_no_stopwords_no_lemmatization_bert.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_no_stopwords_no_lemmatization_glove.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_no_stopwords_no_lemmatization_tfidf.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_no_stopwords_tfidf.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_stemming_bert.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_stemming_glove.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_stemming_no_stopwords_bert.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_stemming_no_stopwords_glove.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_stemming_no_stopwords_tfidf.csv\",\n",
    "    \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/embeddings/preprocessed_stemming_tfidf.csv\"\n",
    "]\n",
    "\n",
    "files = feature_files + embedding_files\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": (\n",
    "        LogisticRegression,\n",
    "        {\"C\": [0.01, 0.1, 1], \"solver\": [\"liblinear\"], \"max_iter\": [100, 200]}\n",
    "    ),\n",
    "    \"RandomForest\": (\n",
    "        RandomForestClassifier,\n",
    "        {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20]}\n",
    "    ),\n",
    "    \"SVC\": (\n",
    "        SVC,\n",
    "        {\"C\": [0.1, 1, 10], \"kernel\": [\"linear\", \"rbf\"]}\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        xgb.XGBClassifier,\n",
    "        {\"n_estimators\": [50, 100, 200], \"max_depth\": [3, 5, 7], \"learning_rate\": [0.01, 0.1, 0.2]}\n",
    "    ),\n",
    "    \"GradientBoosting\": (\n",
    "        GradientBoostingClassifier,\n",
    "        {\"n_estimators\": [50, 100, 200], \"max_depth\": [3, 5, 7], \"learning_rate\": [0.01, 0.1, 0.2]}\n",
    "    ),\n",
    "    \"AdaBoost\": (\n",
    "        AdaBoostClassifier,\n",
    "        {\"n_estimators\": [50, 100, 200], \"learning_rate\": [0.01, 0.1, 1]}\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesClassifier,\n",
    "        {\"n_estimators\": [50, 100, 200], \"max_depth\": [None, 10, 20]}\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def build_1LSTM(vocab_size, max_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "    model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_2LSTM(vocab_size, max_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "    model.add(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_BiLSTM(vocab_size, max_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def build_2BiLSTM(vocab_size, max_length, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length))\n",
    "    model.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "text_models = {\n",
    "    \"1LSTM\": build_1LSTM,\n",
    "    \"2LSTM\": build_2LSTM,\n",
    "    \"BiLSTM\": build_BiLSTM,\n",
    "    \"2BiLSTM\": build_2BiLSTM\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def build_dense_model(input_dim, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation=\"relu\", input_dim=input_dim))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "progress_file = \"progress_log.csv\"\n",
    "if os.path.exists(progress_file):\n",
    "    dfp = pd.read_csv(progress_file)\n",
    "    processed_keys = set(dfp[\"run_key\"].tolist())\n",
    "else:\n",
    "    processed_keys = set()\n",
    "\n",
    "def update_progress_log(new_row):\n",
    "    dfp_new = pd.DataFrame([new_row],\n",
    "        columns=[\"run_key\", \"File\", \"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\"])\n",
    "    if os.path.exists(progress_file):\n",
    "        dfp_new.to_csv(progress_file, mode='a', index=False, header=False)\n",
    "    else:\n",
    "        dfp_new.to_csv(progress_file, index=False)\n",
    "\n",
    "def log_confusion_matrix(y_true, y_pred, run_key, prefix):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    cm_path = f\"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/reports/figures/confusion_matrix_{prefix}_{run_key}.png\"\n",
    "    plt.savefig(cm_path)\n",
    "    mlflow.log_artifact(cm_path)\n",
    "    plt.close()\n",
    "\n",
    "def log_dataset(df, source_file):\n",
    "    try:\n",
    "        ds = mlflow.data.from_pandas(df, source=source_file)\n",
    "        mlflow.data.log_dataset(ds, name=\"dataset\")\n",
    "    except Exception:\n",
    "        mlflow.log_artifact(source_file, artifact_path=\"dataset_csv\")\n",
    "\n",
    "\n",
    "\n",
    "def run_ml_experiments(file):\n",
    "    if not os.path.exists(file):\n",
    "        return\n",
    "    df = pd.read_csv(file)\n",
    "    if \"label\" not in df.columns:\n",
    "        return\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    \n",
    "    is_embedding_file = file in embedding_files\n",
    "\n",
    "    if (file in feature_files) and (\"processed_text\" in df.columns) and (\"lexical_diversity\" in df.columns):\n",
    "        numeric_cols = [\n",
    "            \"lexical_diversity\", \"avg_word_length\", \"sentiment_polarity\",\n",
    "            \"subjectivity\", \"flesch_reading_ease\", \"sentence_length\",\n",
    "            \"named_entity_count\", \"noun_count\", \"verb_count\", \"adj_count\", \"adv_count\"\n",
    "        ]\n",
    "        available_cols = [col for col in numeric_cols if col in df.columns]\n",
    "        X = df[available_cols].values\n",
    "    else:\n",
    "        X = df.drop(columns=[\"label\"]).values\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    name_prefix = os.path.basename(file)\n",
    "    \n",
    "    for m_name, (ModelClass, param_grid) in models.items():\n",
    "        run_key = f\"{name_prefix}_{m_name}\"\n",
    "        if run_key in processed_keys:\n",
    "            continue\n",
    "        with mlflow.start_run(run_name=f\"{m_name}_on_{name_prefix}\"):\n",
    "            mlflow.log_param(\"dataset_file\", file)\n",
    "            mlflow.log_param(\"dataset_type\", \"embedding\" if is_embedding_file else \"feature\")\n",
    "            mlflow.log_param(\"dataset_shape\", X.shape)\n",
    "            if is_embedding_file:\n",
    "                mlflow.log_param(\"embedding_csv\", file)\n",
    "            mlflow.log_param(\"model_type\", m_name)\n",
    "            log_dataset(df, file)\n",
    "            \n",
    "            gs = GridSearchCV(ModelClass(), param_grid, cv=3, scoring=\"accuracy\", n_jobs=1)\n",
    "            gs.fit(X_train, y_train)\n",
    "            best_model = gs.best_estimator_\n",
    "            preds = best_model.predict(X_test)\n",
    "            \n",
    "            acc = accuracy_score(y_test, preds)\n",
    "            prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "            rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "            f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "            \n",
    "            mlflow.log_params(gs.best_params_)\n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "            mlflow.log_metric(\"precision\", prec)\n",
    "            mlflow.log_metric(\"recall\", rec)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            \n",
    "            mlflow.sklearn.log_model(best_model, artifact_path=f\"{m_name}_Model\")\n",
    "            log_confusion_matrix(y_test, preds, run_key, \"ML\")\n",
    "        mlflow.end_run()\n",
    "        update_progress_log([run_key, file, m_name, acc, prec, rec, f1])\n",
    "        processed_keys.add(run_key)\n",
    "\n",
    "\n",
    "def run_dl_text_experiments(file):\n",
    "    if not os.path.exists(file):\n",
    "        return\n",
    "    df = pd.read_csv(file)\n",
    "    if \"label\" not in df.columns or \"processed_text\" not in df.columns:\n",
    "        return\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    \n",
    "    texts = df[\"processed_text\"].fillna(\"\").astype(str).tolist()\n",
    "    \n",
    "    \n",
    "    vocab_size = 10000\n",
    "    max_length = 200\n",
    "    tokenizer_obj = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "    tokenizer_obj.fit_on_texts(texts)\n",
    "    sequences = tokenizer_obj.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "    \n",
    "    X_text = padded\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_text, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    num_classes = len(np.unique(y))\n",
    "    name_prefix = os.path.basename(file)\n",
    "    \n",
    "    for model_name, build_fn in text_models.items():\n",
    "        run_key = f\"{model_name}_{name_prefix}\"\n",
    "        if run_key in processed_keys:\n",
    "            continue\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_on_{name_prefix}\"):\n",
    "            mlflow.log_param(\"dataset_file\", file)\n",
    "            mlflow.log_param(\"model_type\", model_name)\n",
    "            mlflow.log_param(\"text_in_file\", \"Yes\")\n",
    "            mlflow.log_param(\"vocab_size\", vocab_size)\n",
    "            mlflow.log_param(\"max_length\", max_length)\n",
    "            mlflow.log_param(\"num_classes\", num_classes)\n",
    "            mlflow.log_param(\"dataset_shape\", X_text.shape)\n",
    "            log_dataset(df, file)\n",
    "            \n",
    "            model = build_fn(vocab_size, max_length, num_classes)\n",
    "            model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=0)\n",
    "            \n",
    "            loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "            preds_prob = model.predict(X_test)\n",
    "            preds = preds_prob.argmax(axis=1)\n",
    "            \n",
    "            prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "            rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "            f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "            \n",
    "            mlflow.log_metric(\"accuracy\", acc)\n",
    "            mlflow.log_metric(\"precision\", prec)\n",
    "            mlflow.log_metric(\"recall\", rec)\n",
    "            mlflow.log_metric(\"f1_score\", f1)\n",
    "            \n",
    "            mlflow.keras.log_model(model, artifact_path=f\"{model_name}_Model\")\n",
    "            log_confusion_matrix(y_test, preds, run_key, \"DL\")\n",
    "        mlflow.end_run()\n",
    "        update_progress_log([run_key, file, model_name, acc, prec, rec, f1])\n",
    "        processed_keys.add(run_key)\n",
    "\n",
    "def run_dl_embedding_experiments(file):\n",
    "    if not os.path.exists(file):\n",
    "        return\n",
    "    df = pd.read_csv(file)\n",
    "    if \"label\" not in df.columns:\n",
    "        return\n",
    "    df.dropna(inplace=True)\n",
    "    y = df[\"label\"].values\n",
    "    if y.dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "    \n",
    "    X = df.drop(columns=[\"label\"]).values\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    name_prefix = os.path.basename(file)\n",
    "    run_key = f\"DenseNN_{name_prefix}\"\n",
    "    if run_key in processed_keys:\n",
    "        return\n",
    "    with mlflow.start_run(run_name=f\"DenseNN_on_{name_prefix}\"):\n",
    "        mlflow.log_param(\"dataset_file\", file)\n",
    "        mlflow.log_param(\"embedding_file\", \"Yes\")\n",
    "        mlflow.log_param(\"model_type\", \"Dense NN on Embeddings\")\n",
    "        mlflow.log_param(\"dataset_shape\", X.shape)\n",
    "        log_dataset(df, file)\n",
    "        \n",
    "        input_dim = X.shape[1]\n",
    "        num_classes = len(np.unique(y))\n",
    "        model = build_dense_model(input_dim, num_classes)\n",
    "        model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.1, verbose=0)\n",
    "        \n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "        preds = model.predict(X_test).argmax(axis=1)\n",
    "        \n",
    "        prec = precision_score(y_test, preds, average=\"weighted\")\n",
    "        rec = recall_score(y_test, preds, average=\"weighted\")\n",
    "        f1 = f1_score(y_test, preds, average=\"weighted\")\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", acc)\n",
    "        mlflow.log_metric(\"precision\", prec)\n",
    "        mlflow.log_metric(\"recall\", rec)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        \n",
    "        mlflow.keras.log_model(model, artifact_path=\"DenseNN_Model\")\n",
    "        log_confusion_matrix(y_test, preds, run_key, \"DL\")\n",
    "    mlflow.end_run()\n",
    "    update_progress_log([run_key, file, \"Dense NN on Embeddings\", acc, prec, rec, f1])\n",
    "    processed_keys.add(run_key)\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(f):\n",
    "        continue\n",
    "    run_ml_experiments(f)\n",
    "    df_temp = pd.read_csv(f)\n",
    "    if \"processed_text\" in df_temp.columns:\n",
    "        run_dl_text_experiments(f)\n",
    "    if f in embedding_files:\n",
    "        run_dl_embedding_experiments(f)\n",
    "\n",
    "print(\"All experiments completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
