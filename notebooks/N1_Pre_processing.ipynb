{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yogesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/yogesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/yogesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from langdetect import detect\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import inflect\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import contractions\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical values into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded dataset saved: /home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/encoded_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/translated_output.csv')\n",
    "\n",
    "category_encoder = OrdinalEncoder()\n",
    "df[\"category\"] = category_encoder.fit_transform(df[[\"category\"]])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "output_path = \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/encoded_dataset.csv\"\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Encoded dataset saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>deep_translated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category  rating  label  \\\n",
       "0           3.0     5.0      0   \n",
       "1           3.0     5.0      0   \n",
       "2           3.0     5.0      0   \n",
       "3           3.0     1.0      0   \n",
       "4           3.0     5.0      0   \n",
       "...         ...     ...    ...   \n",
       "40427       1.0     4.0      1   \n",
       "40428       1.0     5.0      0   \n",
       "40429       1.0     2.0      1   \n",
       "40430       1.0     1.0      0   \n",
       "40431       1.0     5.0      1   \n",
       "\n",
       "                                                   text_  \\\n",
       "0      Love this!  Well made, sturdy, and very comfor...   \n",
       "1      love it, a great upgrade from the original.  I...   \n",
       "2      This pillow saved my back. I love the look and...   \n",
       "3      Missing information on how to use it, but it i...   \n",
       "4      Very nice set. Good quality. We have had the s...   \n",
       "...                                                  ...   \n",
       "40427  I had read some reviews saying that this bra r...   \n",
       "40428  I wasn't sure exactly what it would be. It is ...   \n",
       "40429  You can wear the hood by itself, wear it with ...   \n",
       "40430  I liked nothing about this dress. The only rea...   \n",
       "40431  I work in the wedding industry and have to wor...   \n",
       "\n",
       "                                    deep_translated_text  \n",
       "0      Love this!  Well made, sturdy, and very comfor...  \n",
       "1      love it, a great upgrade from the original.  I...  \n",
       "2      This pillow saved my back. I love the look and...  \n",
       "3      Missing information on how to use it, but it i...  \n",
       "4      Very nice set. Good quality. We have had the s...  \n",
       "...                                                  ...  \n",
       "40427  I had read some reviews saying that this bra r...  \n",
       "40428  I wasn't sure exactly what it would be. It is ...  \n",
       "40429  You can wear the hood by itself, wear it with ...  \n",
       "40430  I liked nothing about this dress. The only rea...  \n",
       "40431  I work in the wedding industry and have to wor...  \n",
       "\n",
       "[40432 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Languages used in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages found  ['en' 'de' 'af' 'et' 'es' 'fr' 'da' 'vi' 'so' 'ca' 'pt' 'nl' 'no' 'sw'\n",
      " 'tl' 'unknown']\n",
      "Total instances where it was not english 63\n"
     ]
    }
   ],
   "source": [
    "def func(text):\n",
    "    try:\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            return detect(text)\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"  \n",
    "\n",
    "temp = df['text_'].apply(func)\n",
    "\n",
    "print(\"Languages found \",temp.unique())\n",
    "print(\"Total instances where it was not english\",temp[temp != 'en'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The above code return different output everytime but still since there is more than one language it is enough for us do translation regardless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To translate non-English reviews into English using the `deep_translator` library while ensuring progress is saved to avoid reprocessing previously translated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "output_file = \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/translated_output.csv\" \n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    df_translated = pd.read_csv(output_file)\n",
    "    translated_indices = set(df_translated.index)  \n",
    "else:\n",
    "    df_translated = pd.DataFrame()\n",
    "    translated_indices = set()\n",
    "\n",
    "def deep_translate(text):\n",
    "    try:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"Invalid or Empty Text\"\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Deep Translator Error: {e}\")\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if i in translated_indices:\n",
    "        continue  \n",
    "    df.loc[i, 'deep_translated_text'] = deep_translate(df.loc[i, 'text_'])\n",
    "    df.iloc[[i]].to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "    print(f\"Processed row {i+1}/{len(df)}: {df.loc[i, 'deep_translated_text']}\")  \n",
    "\n",
    "print(\"Translation completed or interrupted, progress saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "      <th>deep_translated_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       category  rating  label  \\\n",
       "0           3.0     5.0      0   \n",
       "1           3.0     5.0      0   \n",
       "2           3.0     5.0      0   \n",
       "3           3.0     1.0      0   \n",
       "4           3.0     5.0      0   \n",
       "...         ...     ...    ...   \n",
       "40427       1.0     4.0      1   \n",
       "40428       1.0     5.0      0   \n",
       "40429       1.0     2.0      1   \n",
       "40430       1.0     1.0      0   \n",
       "40431       1.0     5.0      1   \n",
       "\n",
       "                                                   text_  \\\n",
       "0      Love this!  Well made, sturdy, and very comfor...   \n",
       "1      love it, a great upgrade from the original.  I...   \n",
       "2      This pillow saved my back. I love the look and...   \n",
       "3      Missing information on how to use it, but it i...   \n",
       "4      Very nice set. Good quality. We have had the s...   \n",
       "...                                                  ...   \n",
       "40427  I had read some reviews saying that this bra r...   \n",
       "40428  I wasn't sure exactly what it would be. It is ...   \n",
       "40429  You can wear the hood by itself, wear it with ...   \n",
       "40430  I liked nothing about this dress. The only rea...   \n",
       "40431  I work in the wedding industry and have to wor...   \n",
       "\n",
       "                                    deep_translated_text  \n",
       "0      Love this!  Well made, sturdy, and very comfor...  \n",
       "1      love it, a great upgrade from the original.  I...  \n",
       "2      This pillow saved my back. I love the look and...  \n",
       "3      Missing information on how to use it, but it i...  \n",
       "4      Very nice set. Good quality. We have had the s...  \n",
       "...                                                  ...  \n",
       "40427  I had read some reviews saying that this bra r...  \n",
       "40428  I wasn't sure exactly what it would be. It is ...  \n",
       "40429  You can wear the hood by itself, wear it with ...  \n",
       "40430  I liked nothing about this dress. The only rea...  \n",
       "40431  I work in the wedding industry and have to wor...  \n",
       "\n",
       "[40432 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    if i in translated_indices:\n",
    "        continue  \n",
    "    df.loc[i, 'deep_translated_text'] = deep_translate(df.loc[i, 'text_'])\n",
    "    df.iloc[[i]].to_csv(output_file, mode='a', header=not os.path.exists(output_file), index=False)\n",
    "    print(f\"Processed row {i+1}/{len(df)}: {df.loc[i, 'deep_translated_text']}\")  \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages found  ['en' 'da' 'de' 'et' 'fr' 'vi' 'so' 'ca' 'af' 'nl' 'no' 'sw' 'tl'\n",
      " 'unknown']\n",
      "Total instances where it was not english 40\n"
     ]
    }
   ],
   "source": [
    "trans_df = pd.read_csv(\"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/translated_output.csv\")\n",
    "def func(text):\n",
    "    try:\n",
    "        if isinstance(text, str) and text.strip():\n",
    "            return detect(text)\n",
    "        else:\n",
    "            return \"unknown\"\n",
    "    except:\n",
    "        return \"unknown\"  \n",
    "\n",
    "temp = trans_df['deep_translated_text'].apply(func)\n",
    "\n",
    "print(\"Languages found \",temp.unique())\n",
    "print(\"Total instances where it was not english\",temp[temp != 'en'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Since we have not reliable way to detect languages we still managed to half the number of instances and adding this step in our pipline we definitely help "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Pipeline\n",
    "\n",
    "1. Library Initialization\n",
    "- Inflect Engine: Converts numbers to words (e.g., \"4\" â†’ \"four\").\n",
    "- Lemmatizer (WordNetLemmatizer): Reduces words to their base form (e.g., \"running\" â†’ \"run\").\n",
    "- Stemmer (SnowballStemmer): Reduces words to their root (e.g., \"running\" â†’ \"run\"), but more aggressively than lemmatization.\n",
    "- Stopwords Set: Contains common words (e.g., \"the\", \"is\") to be optionally removed.\n",
    "- SymSpell: Used for spelling correction with a predefined dictionary.\n",
    "\n",
    "2. Utility Functions\n",
    "`remove_emojis(text)`\n",
    "- Removes non-ASCII characters (including most emojis).\n",
    "\n",
    "`convert_numbers(text)`\n",
    "- Converts numbers into words using the `inflect` library.\n",
    "\n",
    "`correct_spelling(word)`\n",
    "- Uses SymSpell to find the closest correct spelling for a word.\n",
    "\n",
    "`get_wordnet_pos(word)`\n",
    "- Maps POS (Part of Speech) tags from NLTK to WordNet for more accurate lemmatization.\n",
    "\n",
    "3. Advanced Text Preprocessing Function\n",
    "`advanced_preprocess_text(text, remove_stopwords, use_stemming, use_lemmatization, use_spell_correction, expand_contractions_flag)`\n",
    "- Removes emojis and expands contractions (e.g., \"don't\" â†’ \"do not\").\n",
    "- Removes HTML tags.\n",
    "- Converts text to lowercase.\n",
    "- Converts numbers to words.\n",
    "- Removes punctuation for better tokenization.\n",
    "- Tokenizes the text into words.\n",
    "- Applies optional transformations:\n",
    "  - Spell correction (if enabled).\n",
    "  - Stopword removal (if enabled).\n",
    "  - Stemming or Lemmatization (default: Lemmatization).\n",
    "\n",
    "4. Batch Preprocessing for a DataFrame\n",
    "`advanced_preprocess_texts(df, remove_stopwords, use_stemming, use_lemmatization, use_spell_correction, expand_contractions_flag, filename)`\n",
    "- Applies `advanced_preprocess_text()` to the `deep_translated_text` column of a DataFrame.\n",
    "- Saves the processed text to a CSV file in `../Data/Pre-processed/`.\n",
    "\n",
    "5. Running Preprocessing with Different Configurations\n",
    "- Loads dataset from `encoded_dataset.csv`.\n",
    "- Defines five configurations with different combinations of:\n",
    "  - Stopword removal.\n",
    "  - Lemmatization or stemming.\n",
    "  - Spell correction.\n",
    "  - Contraction expansion.\n",
    "- Processes dataset multiple times using these configurations.\n",
    "- Saves each processed version as a separate CSV file.\n",
    "\n",
    "6. Execution\n",
    "- Runs preprocessing for all configurations.\n",
    "- Saves preprocessed datasets.\n",
    "- Prints completion message.\n",
    "\n",
    "This setup enables efficient text preprocessing while experimenting with different NLP techniques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(text: str) -> str:\n",
    "    \"\"\"Convert numeric digits to their word representation (e.g., '4' -> 'four').\"\"\"\n",
    "    return re.sub(r'\\b\\d+\\b', lambda x: inflect_engine.number_to_words(x.group()), text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have five apples and three oranges'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_numbers(\"I have 5 apples and 3 oranges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(word: str):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS for better lemmatization.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_wordnet_pos(\"running\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file saved: /home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/preprocessed_lemmatization.csv\n",
      "Processed file saved: /home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/preprocessed_no_stopwords_no_lemmatization.csv\n",
      "Processed file saved: /home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/preprocessed_no_stopwords.csv\n",
      "Processed file saved: /home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/preprocessed_stemming_no_stopwords.csv\n",
      "Processed file saved: /home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/preprocessed_stemming.csv\n",
      "All preprocessed datasets saved.\n"
     ]
    }
   ],
   "source": [
    "inflect_engine = inflect.engine()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = \"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/Assets/frequency_dictionary_en_82_765.txt\"\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    raise FileNotFoundError(f\"SymSpell dictionary file not found at {dictionary_path}\")\n",
    "\n",
    "def remove_emojis(text: str) -> str:\n",
    "    \"\"\"Remove all non-ASCII characters (including most emojis).\"\"\"\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "def convert_numbers(text: str) -> str:\n",
    "    \"\"\"Convert numeric digits to their word representation (e.g., '4' -> 'four').\"\"\"\n",
    "    return re.sub(r'\\b\\d+\\b', lambda x: inflect_engine.number_to_words(x.group()), text)\n",
    "\n",
    "def correct_spelling(word: str) -> str:\n",
    "    \"\"\"Correct spelling using SymSpell, returning the closest suggestion if available.\"\"\"\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else word\n",
    "\n",
    "def get_wordnet_pos(word: str):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS for better lemmatization.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def advanced_preprocess_text(\n",
    "    text: str,\n",
    "    remove_stopwords: bool = False,\n",
    "    use_stemming: bool = False,\n",
    "    use_lemmatization: bool = True,\n",
    "    use_spell_correction: bool = False,\n",
    "    expand_contractions_flag: bool = True\n",
    ") -> str:\n",
    "    \"\"\"Preprocess text by removing emojis, expanding contractions, converting numbers, etc.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = remove_emojis(text)\n",
    "    if expand_contractions_flag:\n",
    "        text = contractions.fix(text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = text.lower()\n",
    "    text = convert_numbers(text)\n",
    "\n",
    "    text_clean = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    words = word_tokenize(text_clean)\n",
    "\n",
    "    if use_spell_correction:\n",
    "        words = [correct_spelling(word) for word in words]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    if use_stemming:\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    elif use_lemmatization:\n",
    "        words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "\n",
    "    return \" \".join(words)\n",
    "\n",
    "def advanced_preprocess_texts(\n",
    "    df: pd.DataFrame,\n",
    "    remove_stopwords: bool = False,\n",
    "    use_stemming: bool = False,\n",
    "    use_lemmatization: bool = True,\n",
    "    use_spell_correction: bool = False,\n",
    "    expand_contractions_flag: bool = True,\n",
    "    filename: str = \"processed_output\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Apply the advanced preprocessing pipeline to the 'deep_translated_text' column of the DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"processed_text\"] = df[\"deep_translated_text\"].apply(\n",
    "        lambda x: advanced_preprocess_text(\n",
    "            x,\n",
    "            remove_stopwords,\n",
    "            use_stemming,\n",
    "            use_lemmatization,\n",
    "            use_spell_correction,\n",
    "            expand_contractions_flag\n",
    "        )\n",
    "    )\n",
    "    os.makedirs(\"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed\", exist_ok=True)\n",
    "    filepath = f\"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/{filename}.csv\"\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Processed file saved: {filepath}\")\n",
    "    return df\n",
    "\n",
    "df = pd.read_csv(\"/home/yogesh/mlops/Mlop Projects/Fake Review Detection/data/processed/encoded_dataset.csv\")\n",
    "\n",
    "configurations = [\n",
    "    {\n",
    "        \"remove_stopwords\": False,\n",
    "        \"use_stemming\": False,\n",
    "        \"use_lemmatization\": True,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_lemmatization\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": True,\n",
    "        \"use_stemming\": False,\n",
    "        \"use_lemmatization\": False,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_no_stopwords_no_lemmatization\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": True,\n",
    "        \"use_stemming\": False,\n",
    "        \"use_lemmatization\": True,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_no_stopwords\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": True,\n",
    "        \"use_stemming\": True,\n",
    "        \"use_lemmatization\": False,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_stemming_no_stopwords\"\n",
    "    },\n",
    "    {\n",
    "        \"remove_stopwords\": False,\n",
    "        \"use_stemming\": True,\n",
    "        \"use_lemmatization\": False,\n",
    "        \"use_spell_correction\": False,\n",
    "        \"expand_contractions_flag\": True,\n",
    "        \"filename\": \"preprocessed_stemming\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for config in configurations:\n",
    "    advanced_preprocess_texts(\n",
    "        df,\n",
    "        remove_stopwords=config[\"remove_stopwords\"],\n",
    "        use_stemming=config[\"use_stemming\"],\n",
    "        use_lemmatization=config[\"use_lemmatization\"],\n",
    "        use_spell_correction=config[\"use_spell_correction\"],\n",
    "        expand_contractions_flag=config[\"expand_contractions_flag\"],\n",
    "        filename=config[\"filename\"]\n",
    "    )\n",
    "\n",
    "print(\"All preprocessed datasets saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
