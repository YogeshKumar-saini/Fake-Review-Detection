{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting import-ipynb\n",
      "  Downloading import_ipynb-0.2-py3-none-any.whl (4.0 kB)\n",
      "Requirement already satisfied: IPython in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from import-ipynb) (8.34.0)\n",
      "Requirement already satisfied: nbformat in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from import-ipynb) (5.10.4)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (5.14.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (2.19.1)\n",
      "Requirement already satisfied: decorator in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (5.2.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (3.0.50)\n",
      "Requirement already satisfied: stack_data in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (4.9.0)\n",
      "Requirement already satisfied: exceptiongroup in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (1.2.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from IPython->import-ipynb) (0.19.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from nbformat->import-ipynb) (4.23.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from nbformat->import-ipynb) (2.21.1)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from nbformat->import-ipynb) (5.7.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.4)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (2024.10.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (25.3.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.23.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.36.2)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat->import-ipynb) (4.3.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython->import-ipynb) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from stack_data->IPython->import-ipynb) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from stack_data->IPython->import-ipynb) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/yogesh/python_lab/python_env/lib/python3.10/site-packages (from stack_data->IPython->import-ipynb) (3.0.0)\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(\"pip install import-ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "import re\n",
    "import string\n",
    "import inflect\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "from langdetect import detect\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_translate(text):\n",
    "    try:\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"Invalid or Empty Text\"\n",
    "        return GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Deep Translator Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize necessary components\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = \"../Assets/frequency_dictionary_en_82_765.txt\"\n",
    "if not sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1):\n",
    "    raise FileNotFoundError(f\"SymSpell dictionary file not found at {dictionary_path}\")\n",
    "\n",
    "inflect_engine = inflect.engine()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of a given text, returning 'unknown' if detection fails.\"\"\"\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def convert_numbers(text):\n",
    "    \"\"\"Convert numeric digits to their word representation (e.g., '4' → 'four').\"\"\"\n",
    "    return re.sub(r'\\b\\d+\\b', lambda x: inflect_engine.number_to_words(x.group()), text)\n",
    "\n",
    "def correct_spelling(word):\n",
    "    \"\"\"Correct spelling using SymSpell, returning the closest suggestion if available.\"\"\"\n",
    "    suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "    return suggestions[0].term if suggestions else word\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map NLTK POS tags to WordNet POS for better lemmatization.\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=False, use_stemming=False, use_lemmatization=True):\n",
    "    \"\"\"Preprocess text by normalizing, cleaning, tokenizing, correcting, and optionally lemmatizing/stemming.\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = re.sub(r'<[^>]+>', '', text)  # Remove HTML tags\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "\n",
    "    text = convert_numbers(text)  # Convert numbers to words\n",
    "\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))  # Remove punctuation\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    words = [correct_spelling(word) for word in words]  # Correct spelling\n",
    "\n",
    "    if remove_stopwords:\n",
    "        words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "\n",
    "    if use_stemming:\n",
    "        words = [stemmer.stem(word) for word in words]  # Stemming\n",
    "\n",
    "    if use_lemmatization:\n",
    "        words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]  # Lemmatization\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "# def translate_text(text):\n",
    "#     return deep_translate(text)\n",
    "\n",
    "# def preprocess_single_text(text, remove_stopwords=False, use_stemming=False, use_lemmatization=True):\n",
    "#     text = preprocess_text(text, remove_stopwords, use_stemming, use_lemmatization)\n",
    "#     return text\n",
    "\n",
    "# def feature_extract_single_text(text, vectorizer):\n",
    "#     text_tfidf = vectorizer.transform([text])\n",
    "#     return text_tfidf\n",
    "\n",
    "# def predict_single_text(text, vectorizer, model, remove_stopwords=False, use_stemming=False, use_lemmatization=True):\n",
    "#     translated_text = translate_text(text)\n",
    "#     preprocessed_text = preprocess_single_text(translated_text, remove_stopwords, use_stemming, use_lemmatization)\n",
    "#     text_tfidf = feature_extract_single_text(preprocessed_text, vectorizer)\n",
    "#     prediction = model.predict(text_tfidf)\n",
    "#     return prediction\n",
    "\n",
    "# def run_single_text_pipeline(text):\n",
    "#     vectorizer = joblib.load('vectorizer.pkl')  \n",
    "#     model = joblib.load('model.pkl')  \n",
    "\n",
    "#     prediction = predict_single_text(text, vectorizer, model)\n",
    "\n",
    "#     print(\"✅ Prediction completed successfully!\")\n",
    "#     return prediction\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"Enter the text here for prediction\"\n",
    "# prediction = run_single_text_pipeline(text)\n",
    "# print(f\"Prediction: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
